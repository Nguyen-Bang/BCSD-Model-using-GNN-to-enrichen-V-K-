# Dataset Module Contract

**Module**: `pipeline.dataset`  
**Purpose**: Load preprocessed data and provide PyTorch-compatible batching  
**Owner**: User Story 2 (P2)

---

## Public API

### Class: `BCSDataset(torch.utils.data.Dataset)`

**Description**: PyTorch Dataset for loading preprocessed CFG and instruction sequence data.

**Constructor**: `__init__(data_dir: str, split: str, transform: Optional[Callable] = None)`

**Parameters**:
- `data_dir` (str): Root directory containing preprocessed files
- `split` (str): One of "train", "validation", "test"
- `transform` (Optional[Callable]): Optional transformation function

**Methods**:

#### `__len__() -> int`
Returns total number of samples in the dataset.

#### `__getitem__(idx: int) -> Dict[str, Any]`
Returns a single sample as a dictionary:
```python
{
    "binary_hash": str,
    "cfg": {
        "edge_index": torch.Tensor,  # Shape: [2, num_edges]
        "num_nodes": int
    },
    "instruction_tokens": List[str],
    "metadata": {
        "source_project": str,
        "split": str
    }
}
```

#### `get_split_indices() -> Dict[str, List[int]]`
Returns dictionary mapping split names to sample indices.

---

### Function: `collate_fn(batch: List[Dict]) -> Dict[str, Any]`

**Description**: Custom collate function for DataLoader to handle heterogeneous data.

**Parameters**:
- `batch` (List[Dict]): List of samples from `__getitem__`

**Returns**: Batched dictionary:
```python
{
    "binary_hashes": List[str],  # Length: batch_size
    "graphs": torch_geometric.data.Batch,  # Batched graphs
    "instruction_sequences": torch.Tensor,  # Shape: [batch_size, max_seq_len]
    "attention_masks": torch.Tensor,  # Shape: [batch_size, max_seq_len]
    "padding_lengths": List[int],  # Number of padding tokens per sample
    "metadata": List[Dict]
}
```

**Behavior**:
- Pads instruction sequences to max length in batch
- Creates attention masks (1 for real tokens, 0 for padding)
- Uses PyTorch Geometric's `Batch.from_data_list()` for graph batching
- Preserves metadata for each sample

---

### Function: `create_training_pairs(dataset: BCSDataset, pair_type: str = "function_level") -> pd.DataFrame`

**Description**: Generates positive pairs (semantically identical functions with different compilation) for Siamese contrastive learning.

**Parameters**:
- `dataset` (BCSDataset): Source dataset (must include function-level metadata)
- `pair_type` (str): Type of pairing strategy, default "function_level"

**Returns**: Pandas DataFrame with columns:
```
| sample_id | function_name | variant_A_hash | variant_B_hash | label | compilation_A | compilation_B | split |
|-----------|---------------|----------------|----------------|-------|---------------|---------------|-------|
| 0         | sort          | abc123...      | def456...      | 1     | gcc_O0        | clang_O3      | train |
| 1         | parse         | ghi789...      | jkl012...      | 1     | gcc_O2        | clang_O0      | train |
```

**Pair Generation Strategy (Function-Level Matching)**:
- **Positive pairs ONLY**: Functions with same semantic identity but different compilation
  - Example: `sort_gcc_O0` paired with `sort_clang_O3` (same function, different compiler/optimization)
  - Source: BinaryCorp dataset provides function-level annotations with compilation variants
- **No explicit negative pairs**: Negatives are generated implicitly during training as in-batch negatives
  - For batch containing [`sort_gcc_O0`, `sort_clang_O3`, `parse_gcc_O0`, `parse_clang_O3`, ...]:
    - Positive pair: (`sort_gcc_O0`, `sort_clang_O3`)
    - In-batch negatives for `sort_gcc_O0`: `parse_gcc_O0`, `parse_clang_O3`, all other non-sort functions
- **Label**: Always 1 (positive pairs only; negatives handled by loss function)

**Data Requirements**:
- Dataset must include function-level annotations: `function_name`, `compilation_settings`
- Each function must have at least 2 compilation variants to form pairs
- BinaryCorp format: functions extracted from binaries with compilation metadata

---

## Data Flow

```
1. BCSDataset.__init__()
   ↓ reads data_dir/binaries_index.csv
   ↓ filters by split
   
2. BCSDataset.__getitem__(idx)
   ↓ loads {binary_hash}_cfg.json
   ↓ loads {binary_hash}_seq.json
   ↓ converts to PyTorch tensors
   
3. DataLoader with collate_fn
   ↓ batches samples
   ↓ pads sequences
   ↓ creates attention masks
   
4. Output: Ready for model input
```

---

## Configuration

**Dataset Structure** (expected directory layout):
```
data/
├── binaries_index.csv          # Metadata for all binaries
├── preprocessed/
│   ├── {hash1}_cfg.json
│   ├── {hash1}_seq.json
│   ├── {hash2}_cfg.json
│   ├── {hash2}_seq.json
│   └── ...
└── training_pairs_train.csv    # Generated by create_training_pairs()
```

**`binaries_index.csv` Schema**:
```csv
binary_hash,file_path,source_project,split,file_size_bytes,is_stripped,preprocessing_status
abc123...,/path/to/clamav_clamscan,clamav,train,1024000,False,success
def456...,/path/to/curl_curl,curl,train,2048000,False,success
...
```

---

## Testing Interface

**Unit Tests**:
- `test_dataset_length()`: Verify dataset reports correct number of samples
- `test_getitem_valid()`: Fetch sample and verify structure
- `test_collate_fn_padding()`: Verify sequences are padded correctly
- `test_collate_fn_masks()`: Verify attention masks match padding
- `test_create_pairs_balance()`: Verify positive/negative balance
- `test_create_pairs_no_self()`: Verify no self-pairs (binary1 != binary2)

**Integration Test**:
- Create small test dataset (3 binaries from 2 projects)
- Instantiate BCSDataset and DataLoader
- Iterate through one epoch and verify shapes:
  - `instruction_sequences`: [batch_size, max_seq_len]
  - `attention_masks`: [batch_size, max_seq_len]
  - `graphs.batch`: [total_nodes_in_batch]

---

## Example Usage

```python
from pipeline.dataset import BCSDataset, collate_fn, create_training_pairs
from torch.utils.data import DataLoader

# Create dataset
train_dataset = BCSDataset(
    data_dir="data",
    split="train"
)

print(f"Training samples: {len(train_dataset)}")

# Create DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=16,
    shuffle=True,
    collate_fn=collate_fn,
    num_workers=4,
    pin_memory=True
)

# Iterate through batches
for batch in train_loader:
    graphs = batch["graphs"]  # PyTorch Geometric Batch
    sequences = batch["instruction_sequences"]  # [16, max_len]
    masks = batch["attention_masks"]  # [16, max_len]
    
    print(f"Batch: {sequences.shape}, Graphs: {graphs.num_graphs} graphs")
    break

# Generate training pairs for contrastive learning
pairs_df = create_training_pairs(train_dataset, num_negatives=3)
pairs_df.to_csv("data/training_pairs_train.csv", index=False)
print(f"Generated {len(pairs_df)} training pairs")
```

---

## Performance Specifications

| Operation | Expected Time | Memory |
|-----------|---------------|--------|
| Dataset initialization | <5 seconds | <100 MB |
| Single __getitem__() | <10 ms | <1 MB |
| Batch collation (batch_size=16) | <50 ms | <100 MB |
| Full epoch iteration (1000 samples) | <30 seconds | <2 GB |

**Optimization Notes**:
- Use `num_workers=4` in DataLoader for parallel loading
- Enable `pin_memory=True` for faster GPU transfer
- Consider caching preprocessed tensors in RAM if dataset is small (<10GB)

---

## Validation Rules

1. **On Dataset Creation**:
   - `data_dir` must exist and contain `binaries_index.csv`
   - `split` must be one of: "train", "validation", "test"
   - All binaries in split must have `preprocessing_status == "success"`

2. **On __getitem__**:
   - CFG file must exist for the binary
   - Sequence file must exist for the binary
   - `edge_index` must have shape [2, num_edges]
   - `instruction_tokens` must not be empty

3. **On Collation**:
   - All samples in batch must have valid data
   - Padded sequences must have same length within batch
   - Attention masks must sum to number of real tokens per sample

4. **On Pair Generation**:
   - No self-pairs: `binary1_hash != binary2_hash`
   - Positive pairs: both binaries from same `source_project`
   - Negative pairs: binaries from different `source_project`
   - `label` == 1 iff `is_positive_pair` == True

---

## Error Handling

- Missing CFG/sequence file → Skip sample and log warning (or raise error if strict mode)
- Invalid JSON → Raise `ValueError` with file path
- Shape mismatch in collation → Raise `RuntimeError` with details
- Empty dataset after filtering → Raise `ValueError("No valid samples in split")`

---

## Dependencies

**Required Libraries**:
- torch >= 2.0
- torch_geometric >= 2.3
- pandas >= 1.5
- numpy >= 1.24
