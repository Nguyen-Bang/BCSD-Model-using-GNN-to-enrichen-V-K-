# Research Session: December 14, 2025

**Date**: 2025-12-14  
**Project**: BCSD Model using GNN to Enrich V-K  
**Phase**: Implementation Phase

---

## Session Goals

- [ ] Define objectives for today's work
- [ ] Review current implementation status
- [ ] Plan next tasks from tasks.md

---

## Current Status

### Completed Phases
- ✅ Phase 1: Setup (T001-T009)
- ✅ Phase 2: Foundational - Test Binary Validation (T010-T014)
- ✅ Phase 3: User Story 1 - Preprocessing (T015-T026)
- ✅ Phase 4: User Story 2 - Dataset Implementation (T027-T036)
- ✅ Phase 5: User Story 3 - GNN Encoder (T037-T045)
- ✅ Phase 6: User Story 4 - Custom Attention (T046-T058)
- ✅ Phase 7: User Story 5 - BERT Integration (T059-T070)
- ✅ Phase 8: User Story 6 - Demonstration Notebook (T071-T073)
- ✅ Phase 9: User Story 7 - Training Infrastructure (T074-T089)
- ✅ Phase 10: User Story 8 - Vectorization & Inference (T090-T101)
- ⚠️ Phase 13: Polish (T118-T119 complete, T120-T121 deferred, T122-T123 complete)

### Pending Phases
- ✅ Phase 11: User Story 9 - Session Management (T102-T109 complete, T110 deferred)
- ⏳ Phase 12: User Story 10 - Document Review Agent (T111-T117)

### Active Context
- **COMPLETED**: Phase 11 (User Story 9 - Session Management System)
- All core session management functionality implemented and tested
- Next: Phase 12 (User Story 10 - Document Review Agent) or continue with training/evaluation

---

## Actions Taken

### Morning Session
*Document activities here as you work*

- Started session log for 2025-12-14
- Extracted complete pipeline dataflow and format specifications
- Documented data formats for all 8 phases (Preprocessing → Inference)

### Afternoon Session
*Continue logging activities*

- Implemented complete User Story 9 (Session Management System)
  - Created `scripts/start_session.sh` with structured session template (T102-T103)
  - Created `scripts/end_session.sh` with summary extraction and LaTeX integration (T104-T105)
  - Set up complete thesis LaTeX structure (T106-T107)
    - `thesis/main.tex` - Main document
    - `thesis/methodology.tex` - With session log section
    - All chapter templates (introduction, related work, architecture, etc.)
    - Bibliography structure
  - Tested session creation (T108) ✓
  - Tested session closeout with LaTeX extraction (T109) ✓
  - Created comprehensive documentation in `protocols/SESSION_MANAGEMENT_README.md`
- Phase 11 (US9) Status: **COMPLETE** ✅

- **Enhanced end_session.sh script** (based on user feedback):
  - Made LaTeX extraction **mandatory** (removed --no-latex option)
  - Added prompt for **thesis-relevant insights** (research contributions, not just implementation details)
  - Implemented **automatic git workflow**: stage changes, commit with session summary, push to remote
  - Added confirmation prompt before pushing to repository
  - Updated documentation to reflect research-focused approach

---

## Pipeline Dataflow Analysis

### Complete Pipeline Overview

The BCSD pipeline consists of 8 sequential phases, each with well-defined input/output formats:

```
Binary Executable → [1] Preprocessing → [2] Dataset Loading → [3] Collation → 
[4] GNN Encoding → [5] BERT Processing → [6] Training → [7] Vectorization → [8] Similarity Search
```

---

### Phase 1: Preprocessing (angr CFG Extraction + Tokenization)

**Module**: `preprocessing/extract_features.py`, `preprocessing/batch_preprocess.py`

**Input Format**:
- Binary executable file (ELF/PE format)
- Example: `Dataset-1/clamav/arm32-gcc-4.8-O0_clambc`

**Processing**:
1. **angr CFGFast** extracts Control Flow Graph
2. Disassembles basic blocks → assembly instructions
3. Tokenizes instructions → token strings
4. Groups by function (isolates functions from single binary)

**Output Format**: `data/preprocessed/{hash}.json`
```json
{
  "binary_hash": "sha256_hash",
  "binary_path": "path/to/binary",
  "binary_name": "filename",
  "functions": [
    {
      "function_name": "main",
      "function_addr": "0x401000",
      "nodes": [
        {
          "id": 0,
          "addr": "0x401000",
          "instructions": ["push rbp", "mov rbp, rsp", "..."],
          "token_ids": [101, 234, 567, ...],
          "attention_mask": [1, 1, 1, ...]
        }
      ],
      "edges": [[0, 1], [1, 2], [1, 3]]  // CFG edges
    }
  ],
  "metadata": {
    "total_nodes": 67,
    "total_edges": 86,
    "total_functions": 5,
    "processing_time": 3.45
  }
}
```

**Also Generated**: `data/metadata.csv`
```csv
file_hash,binary_name,binary_path,project,function_name,compiler,optimization,split_set
abc123...,test_gnn_gcc_O0,path/to/binary,test,main,gcc,O0,train
```

---

### Phase 2: Dataset Loading (Dynamic Pairing)

**Module**: `dataset/code_dataset.py`

**Input Format**:
- `data/metadata.csv` (index of all binaries)
- `data/preprocessed/*.json` (actual CFG data)

**Processing**:
1. Load metadata CSV, filter by split (train/val/test)
2. Group binaries by `(project, function_name)` → positive pairs
3. Dynamically sample 2 binaries from same function group per `__getitem__`

**Output Format**: Single sample (pair of binaries)
```python
{
  "binary_1": {
    "tokens": ["push", "mov", "rbp", ...],  # Token strings
    "edges": [[0, 1], [1, 2]],
    "node_count": 67,
    "edge_count": 86,
    "file_hash": "abc123...",
    "function_name": "main"
  },
  "binary_2": {
    "tokens": ["push", "mov", "rsp", ...],
    "edges": [[0, 1], [0, 2]],
    "node_count": 45,
    "edge_count": 52,
    "file_hash": "def456...",
    "function_name": "main"
  },
  "label": 1,  # Positive pair (same function)
  "metadata": {
    "project": "openssl",
    "function_name": "main",
    "compiler_1": "gcc", "opt_1": "O0",
    "compiler_2": "gcc", "opt_2": "O3"
  }
}
```

---

### Phase 3: Collation (Batch Construction)

**Module**: `dataset/collate.py`

**Input Format**: List of samples from Phase 2 (batch_size samples)

**Processing**:
1. Separate binary_1 and binary_2 data
2. Pad token sequences to max_len in batch
3. Create attention masks (1 for real tokens, 0 for padding)
4. Keep edges as separate lists (variable-size graphs)

**Output Format**: Batched dictionary
```python
{
  # Binary 1 data
  "binary_1_tokens": List[List[str]],  # [batch_size, max_len] padded
  "binary_1_attention_mask": torch.Tensor,  # [batch_size, max_len]
  "binary_1_edges": List[List[List[int]]],  # List of edge lists
  "binary_1_node_counts": torch.Tensor,  # [batch_size]
  
  # Binary 2 data  
  "binary_2_tokens": List[List[str]],  # [batch_size, max_len]
  "binary_2_attention_mask": torch.Tensor,  # [batch_size, max_len]
  "binary_2_edges": List[List[List[int]]],
  "binary_2_node_counts": torch.Tensor,  # [batch_size]
  
  # Labels
  "labels": torch.Tensor,  # [batch_size] all 1s (positive pairs)
  "metadata": List[Dict]  # Pair metadata
}
```

---

### Phase 4: GNN Encoding (Graph → Fixed-size Summary)

**Module**: `models/gnn_encoder.py` (GATEncoder)

**Input Format** (per binary):
```python
node_features: torch.Tensor  # [total_nodes, 768] - averaged token embeddings
edge_index: torch.Tensor     # [2, total_edges] - COO format
batch: torch.Tensor          # [total_nodes] - graph assignment
```

**Processing**:
1. 3-layer GAT with 4 attention heads per layer
2. Message passing: nodes aggregate neighbor information
3. Global pooling: attention-weighted mean over all nodes

**Output Format**:
```python
graph_summary: torch.Tensor  # [batch_size, graph_dim]
# graph_dim ∈ {128, 256, 512} configurable
```

---

### Phase 5: BERT Processing (Custom KV-Prefix Attention)

**Module**: `models/bert_encoder.py` (BERTWithGraphPrefix)

**Input Format**:
```python
input_ids: torch.Tensor        # [batch_size, seq_len] - token IDs
attention_mask: torch.Tensor   # [batch_size, seq_len]
graph_summary: torch.Tensor    # [batch_size, graph_dim] from GNN
```

**Processing**:
1. Project graph_summary to K/V prefixes (separate linear layers)
2. In each BERT layer (12 layers):
   - Split prefix into multi-head format [batch, 12 heads, 1, 64]
   - Concatenate prefix_k to sequence keys (prepend position 0)
   - Concatenate prefix_v to sequence values
   - Extend attention mask to include prefix (always attended)
   - Compute attention with extended K/V
3. Extract [CLS] token (position 0) from final layer

**Output Format**:
```python
{
  "embeddings": torch.Tensor,     # [batch_size, 768] - [CLS] token
  "hidden_states": torch.Tensor,  # [batch_size, seq_len, 768]
  "mlm_logits": torch.Tensor      # [batch_size, seq_len, vocab_size]
}
```

---

### Phase 6: Training (Siamese + Joint Loss)

**Module**: `training/trainer.py`

**Input Format**: Batch from Phase 3 (collated pairs)

**Processing**:
1. **Siamese Forward Pass**:
   - Process binary_1 through model → embeddings_1 [batch, 768]
   - Process binary_2 through model → embeddings_2 [batch, 768]
   - Same model weights for both (weight sharing)

2. **Loss Computation**:
   - **MLM Loss**: Masked language model loss on tokens
   - **Contrastive Loss**: InfoNCE with temperature=0.07
     - Positive: (embeddings_1, embeddings_2) same function
     - Negatives: Other samples in batch
   - **Joint Loss**: `MLM + λ*Contrastive` (λ ∈ {0.3, 0.5, 0.7})

3. **Optimization**:
   - AdamW optimizer, lr=2e-5
   - Gradient clipping (max_norm=1.0)
   - Backpropagation, parameter update

**Output Format**: Trained model checkpoint
```python
{
  "model_state_dict": OrderedDict,  # Model weights
  "optimizer_state_dict": dict,     # Optimizer state
  "epoch": 10,
  "train_loss": 0.245,
  "val_loss": 0.312,
  "config": {...}
}
```

**Also Generated**: `logs/training_metrics.csv`
```csv
epoch,train_loss,val_loss,mlm_loss,contrastive_loss,total_loss
1,0.456,0.523,0.234,0.222,0.456
2,0.398,0.487,0.201,0.197,0.398
...
```

---

### Phase 7: Vectorization (Inference)

**Module**: `inference/vectorizer.py`

**Input Format**:
- Binary executable file
- Trained model checkpoint
- Tokenizer vocabulary

**Processing**:
1. Extract CFG with angr (same as Phase 1)
2. Tokenize instructions
3. Forward pass through trained model (NO gradients)
4. Extract [CLS] embedding

**Output Format**:
```python
embedding: np.ndarray  # (768,) - function fingerprint
```

**Saved as**: `embeddings/{hash}.npy`

---

### Phase 8: Similarity Search (Mathematical)

**Module**: `inference/similarity.py`

**Input Format**:
```python
query_embedding: np.ndarray        # (768,) - query function
database_embeddings: Dict[str, np.ndarray]  # {hash: embedding(768,)}
```

**Processing**:
1. **Cosine Similarity**: `(A·B) / (||A|| × ||B||)`
2. Vectorized computation for all database embeddings
3. Sort by similarity score
4. Return top-K matches

**Output Format**:
```python
results: List[Tuple[str, float]] = [
  ("hash_1", 0.987),  # Most similar
  ("hash_2", 0.923),
  ("hash_3", 0.891),
  ...
]
```

**Performance**: <1 second for 10,000 embeddings (pure NumPy, CPU-only)

---

### Key Data Format Conventions

1. **Graph Representation**: 
   - Nodes: Sequential IDs [0, N-1]
   - Edges: List of [src_id, dst_id] pairs
   - PyTorch Geometric COO format for batching

2. **Token Sequences**:
   - Special tokens: [PAD]=0, [CLS]=101, [SEP]=102, [MASK]=103, [UNK]=104
   - Max sequence length: 512 tokens
   - Padding: Right-padded with [PAD]

3. **Embeddings**:
   - Fixed size: 768 dimensions (BERT hidden size)
   - Normalized for cosine similarity
   - Stored as float32 numpy arrays

4. **File Naming**:
   - Preprocessed: `{sha256_hash}.json`
   - Embeddings: `{sha256_hash}.npy`
   - Checkpoints: `model_epoch_{N}_valloss_{loss:.4f}.pt`

---

## Technical Decisions

### Decision Log
*Record any architectural or implementation decisions*

1. **Decision**: Function-level CFG extraction (not whole-binary)
   - **Rationale**: Enables fine-grained similarity at function level
   - **Impact**: Each JSON contains multiple functions, isolated graphs

2. **Decision**: Dynamic pairing in Dataset (no pre-computed pairs CSV)
   - **Rationale**: More flexible, reduces storage, enables on-the-fly augmentation
   - **Impact**: Pairing happens in `__getitem__`, metadata.csv groups by function

3. **Decision**: Separate K/V projections for graph prefix
   - **Rationale**: More expressive than shared projection
   - **Impact**: 2× parameters for prefix (graph_to_k, graph_to_v)

4. **Decision**: Mandatory LaTeX extraction with thesis-relevant insights
   - **Rationale**: Bachelor thesis requires continuous academic documentation, not optional
   - **Impact**: Every session contributes to methodology chapter; focus on research contributions

5. **Decision**: Automatic git commit and push in end_session.sh
   - **Rationale**: Streamlines workflow, ensures changes are backed up immediately
   - **Impact**: Reduces friction in documentation process; confirmation prompt prevents accidental commits

---

## Code Changes

### Files Modified
*List files changed with brief description*

- `scripts/start_session.sh`: Session creation script with template generation
- `scripts/end_session.sh`: Session closeout with LaTeX extraction
- `thesis/main.tex`: Main thesis document structure
- `thesis/methodology.tex`: Methodology chapter with session log integration
- `thesis/introduction.tex`: Introduction chapter template
- `thesis/related_work.tex`: Related work chapter template
- `thesis/architecture.tex`: Architecture chapter template
- `thesis/experiments.tex`: Experiments chapter template
- `thesis/results.tex`: Results chapter template
- `thesis/discussion.tex`: Discussion chapter template
- `thesis/conclusion.tex`: Conclusion chapter template
- `thesis/appendix_code.tex`: Code appendix template
- `thesis/appendix_data.tex`: Data appendix template
- `thesis/references.bib`: Bibliography file
- `protocols/SESSION_MANAGEMENT_README.md`: Complete documentation for US9

### Tests Added/Modified
*List test files affected*

- 

---

## Issues Encountered

### Blockers
*Critical issues preventing progress*

1. 

### Warnings/Notes
*Non-blocking issues or observations*

1. 

---

## Outcomes

### Completed Tasks
*Reference task IDs from tasks.md*

- [X] T102: Create scripts/start_session.sh with session start logic
- [X] T103: Create session template in start_session.sh
- [X] T104: Create scripts/end_session.sh with session end logic
- [X] T105: Implement LaTeX extraction in end_session.sh
- [X] T106: Create thesis/main.tex with basic structure
- [X] T107: Create thesis/methodology.tex with sessions section
- [X] T108: Test start_session.sh → verify creates protocols/session_{date}.md
- [X] T109: Test end_session.sh → verify thesis/methodology.tex updated

### Validation Results
*Test results, metrics, observations*

- 

### Knowledge Gained
*New insights or learnings*

- **Session Management Best Practices**: Learned effective patterns for research documentation
  - Structured templates reduce friction in documentation
  - Automated extraction to LaTeX saves significant time
  - Prompt-based closeout ensures completeness
  
- **Bash Script Design**: 
  - Color-coded output improves user experience
  - Argument parsing with `--flags` better than positional arguments
  - Using `awk` for structured text manipulation is powerful
  
- **LaTeX Integration**:
  - Automatic extraction enables continuous documentation
  - Session logs translate well to methodology chapters
  - Chronological development logs add transparency to research

- **Pipeline Dataflow Documentation**:
  - Each phase has clear input/output formats
  - Data format specifications critical for debugging
  - Function-level CFG isolation more flexible than whole-binary

---

## Next Session Planning

### Immediate Next Steps
1. 
2. 
3. 

### Questions for Review
- 

### Dependencies Needed
- 

---

## Thesis Integration Notes

### Methodology Chapter
*Key points to extract for thesis/methodology.tex*

- 

### Results/Discussion
*Observations relevant for results chapter*

- 

### Figures/Tables Generated
*Reference any visualizations created*

- 

---

## Session Metrics

- **Duration**: ~3 hours (pipeline analysis + US9 implementation)
- **Tasks Completed**: 8 (T102-T109)
- **Tests Passing**: ✅ Session management workflow validated
- **Code Quality**: ✅ Scripts tested and documented
- **Files Created**: 17 new files (~1500 lines total)
- **Documentation**: 3 comprehensive markdown files

---

## End-of-Session Summary

**Summary**: Implemented complete User Story 9 (Session Management System) with start/end scripts, thesis LaTeX structure, and automated extraction. Also documented comprehensive pipeline dataflow analysis for all 8 phases.

**Key Achievements**:
1. Created `scripts/start_session.sh` with structured Markdown template generation (T102-T103)
2. Created `scripts/end_session.sh` with interactive closeout and LaTeX extraction (T104-T105)
3. Set up complete thesis LaTeX structure with 12 files: main.tex, methodology.tex, all chapter templates, bibliography (T106-T107)
4. Tested and validated session management workflow (T108-T109)
5. Documented complete pipeline dataflow with exact data formats for all 8 phases (Preprocessing → Inference)
6. Created comprehensive documentation: SESSION_MANAGEMENT_README.md and US9_IMPLEMENTATION_SUMMARY.md

**Blockers Remaining**:
- None (T110 LaTeX compilation deferred - requires pdflatex installation)

**Next Session Priority**:
- Continue implementation and testing
- Option 2: Begin model training on Dataset-1 with full preprocessing pipeline
- Option 3: Continue with Phase 13 polish tasks (T120-T121 - full pipeline reproduction)
