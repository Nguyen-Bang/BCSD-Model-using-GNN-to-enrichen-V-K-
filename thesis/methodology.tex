\chapter{Methodology and Development Process}

\section{Overview}

This chapter documents the research methodology and iterative development process
employed in building the BCSD (Binary Code Similarity Detection) model. Following
a constitution-based research approach, we emphasize reproducibility, documentation,
and systematic experimentation throughout the development lifecycle.

\subsection{Research Philosophy}

The development follows key principles:
\begin{itemize}
    \item \textbf{Documentation First:} All design decisions are documented before implementation
    \item \textbf{Reproducible Pipeline:} Every experiment can be reproduced with fixed random seeds
    \item \textbf{Modular Architecture:} Components are independently testable and replaceable
    \item \textbf{Iterative Validation:} Each phase is validated before proceeding to the next
\end{itemize}

\subsection{Development Pipeline}

The BCSD model development follows an 8-phase pipeline:

\begin{enumerate}
    \item \textbf{Preprocessing:} Binary disassembly and CFG extraction using angr
    \item \textbf{Tokenization:} Custom assembly instruction tokenization with domain vocabulary
    \item \textbf{Dataset Construction:} Dynamic positive pair sampling for Siamese training
    \item \textbf{GNN Encoding:} Graph Attention Networks for CFG summarization
    \item \textbf{Custom Attention:} Key-Value prefix injection mechanism
    \item \textbf{BERT Integration:} Deep graph-prefix fusion across all transformer layers
    \item \textbf{Training:} Joint MLM and contrastive learning with configurable loss weights
    \item \textbf{Inference:} Vectorization and similarity search using cosine distance
\end{enumerate}

\section{Phase 1: Binary Preprocessing and CFG Extraction}

\subsection{angr CFGFast Analysis}

We employ angr's CFGFast analysis for efficient and accurate control flow graph
extraction without the overhead of full symbolic execution. The choice of CFGFast
over CFGEmulated is motivated by:

\begin{itemize}
    \item \textbf{Speed:} 10-100× faster than symbolic execution
    \item \textbf{Accuracy:} Pattern-based approach yields high-quality CFGs
    \item \textbf{Scalability:} Can process large binaries in reasonable time
\end{itemize}

The extraction process is function-centric, isolating individual functions to enable
fine-grained similarity detection at the function level rather than whole-binary level.

\subsection{Data Format Design}

The preprocessed data format follows a unified JSON schema:

\begin{lstlisting}[language=json, caption={Unified JSON format for preprocessed binaries}]
{
  "binary_hash": "sha256_hash",
  "functions": [
    {
      "function_name": "main",
      "nodes": [
        {
          "id": 0,
          "instructions": ["push rbp", "mov rbp, rsp"],
          "token_ids": [101, 234, 567]
        }
      ],
      "edges": [[0, 1], [1, 2]]
    }
  ]
}
\end{lstlisting}

This format enables efficient loading during training while preserving both sequential
(instructions) and structural (edges) information.

\section{Phase 2: Custom Assembly Tokenization}

Unlike natural language, assembly code exhibits unique characteristics that require
specialized tokenization strategies. Our custom tokenizer implements:

\begin{itemize}
    \item \textbf{Opcode-Register Separation:} Splitting instructions into semantic units
    \item \textbf{Frequency-Based Vocabulary:} Top-K most common tokens (K=5000)
    \item \textbf{Special Token Handling:} BERT-compatible special tokens ([CLS], [SEP], etc.)
    \item \textbf{Semantic Grouping:} Grouping similar instructions (e.g., conditional jumps)
\end{itemize}

The vocabulary size of 5000 tokens balances coverage and model efficiency, capturing
95\% of instruction patterns in our dataset while maintaining reasonable embedding
table size.

\section{Phase 3: Siamese Training with Dynamic Pairing}

The training architecture employs Siamese networks with dynamic positive pair sampling.
Unlike traditional fixed-pair approaches, our dataset implementation:

\begin{enumerate}
    \item Groups binaries by (project, function\_name)
    \item Samples 2 binaries from same group per training iteration
    \item Ensures diversity through random sampling across compilation variants
\end{enumerate}

This approach increases effective training data size without storing pre-computed pairs,
enabling efficient augmentation and reducing storage requirements.

\section{Phase 4-6: GNN and Custom Attention Integration}

\subsection{Graph Attention Network Architecture}

The GNN encoder employs a 3-layer Graph Attention Network with multi-head attention:

\begin{itemize}
    \item \textbf{Layer 1:} Input node features $\rightarrow$ hidden dimension 256
    \item \textbf{Layer 2-3:} Hidden $\rightarrow$ hidden with residual connections
    \item \textbf{Pooling:} Attention-weighted global pooling
    \item \textbf{Output:} Fixed-size graph summary (configurable: 128/256/512 dims)
\end{itemize}

\subsection{Key-Value Prefix Attention Mechanism}

The core innovation of this thesis is the custom KV-prefix attention mechanism that
injects graph knowledge into BERT's attention layers. Unlike concatenation-based fusion,
this approach:

\begin{enumerate}
    \item Projects graph summary to separate Key and Value prefix vectors
    \item Injects prefixes at \textbf{every attention layer} (deep fusion)
    \item Extends attention mask to include prefix position
    \item Maintains gradient flow to graph encoder through prefix projections
\end{enumerate}

Mathematically, for each attention layer $l$:

\begin{align}
K_{prefix}^{(l)} &= W_K^{(l)} \cdot \text{graph\_summary} \\
V_{prefix}^{(l)} &= W_V^{(l)} \cdot \text{graph\_summary} \\
K_{total}^{(l)} &= [K_{prefix}^{(l)}; K_{sequence}^{(l)}] \\
V_{total}^{(l)} &= [V_{prefix}^{(l)}; V_{sequence}^{(l)}] \\
\text{Attention}^{(l)} &= \text{softmax}\left(\frac{Q K_{total}^T}{\sqrt{d_k}}\right) V_{total}
\end{align}

This design ensures that every token in the sequence can attend to the graph summary,
enabling rich cross-modal interaction.

\section{Phase 7: Joint Training Objective}

The model is trained with a joint objective combining:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{MLM} + \lambda \cdot \mathcal{L}_{contrastive}
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_{MLM}$: Masked Language Model loss for instruction prediction
    \item $\mathcal{L}_{contrastive}$: InfoNCE contrastive loss with temperature $\tau = 0.07$
    \item $\lambda \in \{0.3, 0.5, 0.7\}$: Configurable loss weight for experimentation
\end{itemize}

The dual objective ensures the model learns both:
\begin{enumerate}
    \item Local instruction semantics (via MLM)
    \item Global function-level similarity (via contrastive learning)
\end{enumerate}

\section{Phase 8: Inference and Similarity Search}

The inference pipeline is designed for efficiency:

\begin{enumerate}
    \item \textbf{Vectorization:} Extract 768-dimensional embeddings from trained model
    \item \textbf{Storage:} Save embeddings as NumPy arrays (.npy format)
    \item \textbf{Search:} CPU-only cosine similarity using vectorized NumPy operations
\end{enumerate}

Performance benchmarks demonstrate sub-second search times for 10,000 embeddings,
validating the practical applicability of the approach.

\section{Research Methodology and Development Log}

This section documents the iterative development process of the BCSD model,
providing transparency into the research methodology and implementation decisions.

% Session logs are automatically appended here by end\_session.sh


\subsection{Development Session: 2025-12-14}

\textbf{Summary:} Complete implementation of session management system and code cleanup

\textbf{Duration:} Multiple sessions throughout the day

\textbf{Project Status - Completed Phases:}
\begin{itemize}
  \item Phase 1: Setup (T001-T009)
  \item Phase 2: Foundational - Test Binary Validation (T010-T014)
  \item Phase 3: User Story 1 - Preprocessing (T015-T026)
  \item Phase 4: User Story 2 - Dataset Implementation (T027-T036)
  \item Phase 5: User Story 3 - GNN Encoder (T037-T045)
  \item Phase 6: User Story 4 - Custom Attention (T046-T058)
  \item Phase 7: User Story 5 - BERT Integration (T059-T070)
  \item Phase 8: User Story 6 - Demonstration Notebook (T071-T073)
  \item Phase 9: User Story 7 - Training Infrastructure (T074-T089)
  \item Phase 10: User Story 8 - Vectorization \& Inference (T090-T101)
\end{itemize}

\textbf{Implementation Work:}
\begin{itemize}
  \item Implemented complete User Story 9 (Session Management System)
  \item Created scripts/start\_session.sh with structured session template (T102-T103)
  \item Created scripts/end\_session.sh with summary extraction and LaTeX integration (T104-T105)
  \item Set up complete thesis LaTeX structure (T106-T107)
  \item Enhanced end\_session.sh script with automatic git workflow: stage changes, commit with session summary, push to remote
  \item Comprehensive code cleanup: removed duplicate hash functions, unified JSON creators, cleaned imports
  \item Updated .gitignore to properly track all implementation files while excluding generated outputs
  \item Modified end\_session.sh to use git add -A for comprehensive file tracking
\end{itemize}

\textbf{Architecture and Design Decisions:}
\begin{itemize}
  \item \textbf{Pipeline Architecture:} 8-phase sequential pipeline from binary executable to similarity search
  \item \textbf{Data Format Design:} Unified JSON schema with binary\_hash, functions array, nodes with token\_ids and attention\_mask, edges as CFG edge list
  \item \textbf{angr CFGFast Parameters:} normalize=True for non-overlapping basic blocks (crucial for consistent GNN node embeddings), force\_complete\_scan=False to reduce noise
  \item \textbf{Siamese Training Architecture:} Same model instance processes multiple inputs sequentially within batch with shared weights
  \item \textbf{Session Management:} Mandatory LaTeX extraction for thesis documentation, automatic protocol parsing, git workflow automation
\end{itemize}

\textbf{Technical Specifications:}
\begin{itemize}
  \item \textbf{Preprocessing Output:} data/preprocessed/\{hash\}.json containing binary\_hash, functions with nodes (id, addr, instructions, token\_ids, attention\_mask), edges as CFG adjacency, metadata (total\_nodes, total\_edges, total\_functions, processing\_time)
  \item \textbf{Dataset Loading:} metadata.csv indexes all binaries, dynamic pairing groups by (project, function\_name)
  \item \textbf{Collation Format:} Heterogeneous batching handles variable-length sequences (padding) and variable-size graphs (separate lists with batch indices)
  \item \textbf{GNN Input:} node\_features [total\_nodes, 768], edge\_index [2, total\_edges] in COO format, batch tensor for graph assignment
  \item \textbf{BERT with Graph Prefix:} Projects graph\_summary to K/V prefixes, concatenates to sequence K/V in all 12 layers, extends attention mask for prefix
  \item \textbf{Training Loss:} Joint loss = MLM + $\lambda$ * Contrastive, InfoNCE with temperature=0.07, $\lambda \in \{0.3, 0.5, 0.7\}$
\end{itemize}

\textbf{Research Insights:}
\begin{itemize}
  \item Code cleanup improved maintainability: eliminated all duplicate functions (compute\_hash, create\_unified\_json)
  \item Session management system enables continuous thesis documentation from day one
  \item All module imports verified working after cleanup
  \item .gitignore properly configured: tracks implementation code, excludes Dataset-1/, generated JSON files, model checkpoints
\end{itemize}


\subsection{Development Session: 2025-12-16}

\textbf{Summary:} Enhanced CFG extraction with visualization and edge semantics

\textbf{Duration:} Morning session

\textbf{Implementation Work:}
\begin{itemize}
  \item Added --visualize flag to preprocessing/extract\_features.py using angr-utils for CFG plot generation
  \item Updated edge extraction to include jumpkind metadata (e.g., Ijk\_Call, Ijk\_Ret, Ijk\_Boring) for control flow semantics
  \item Simplified test\_binaries/test\_gnn.c to reduce CFG complexity (removed stdio.h and printf)
  \item Achieved significant CFG reduction: Nodes 67→41, Edges 86→44
  \item Deleted obsolete test\_binaries/test\_disassembly.py
  \item Successfully generated CFG visualization: .gemini/tmp/test\_output/test\_gnn\_gcc\_O0\_cfg.png
\end{itemize}

\textbf{Architecture and Design Decisions:}
\begin{itemize}
  \item \textbf{CFG Extraction Strategy - normalize=True:} Ensures non-overlapping basic blocks, crucial for consistent GNN node embeddings. Without normalization, overlapping blocks would create ambiguous node representations.
  \item \textbf{CFG Extraction Strategy - force\_complete\_scan=False:} Reduces noise and processing time by analyzing only code reachable from entry points. Prevents angr from treating data sections as code.
  \item \textbf{Entry Points:} Starting addresses (e.g., \_start, function starts) from which control flow analysis begins. Critical for ensuring we analyze reachable code only.
  \item \textbf{Edge Metadata - jumpkind:} Captures semantic relationships (calls, returns, branches) for potential use in GNN edge attributes. Enables the model to distinguish between different types of control flow.
  \item \textbf{Visualization Strategy:} Solid lines for standard flow (Ijk\_Boring, Ijk\_Call), dotted lines for analysis artifacts (Ijk\_FakeRet), color-coded by edge type (Blue=Call, Green=FakeRet, Black=Standard).
\end{itemize}

\textbf{Technical Specifications:}
\begin{itemize}
  \item \textbf{angr.analyses.CFGFast parameters:} normalize creates clean graphs for GNNs, force\_complete\_scan=False avoids analyzing data as code
  \item \textbf{Edge jumpkind types:} Ijk\_Call (function calls), Ijk\_Ret (returns), Ijk\_Boring (standard control flow), Ijk\_FakeRet (analysis artifacts)
  \item \textbf{Visualization dependencies:} System package graphviz required for CFG plotting, but extraction runs fine without it
  \item \textbf{CFG complexity metrics:} Simplified test binary achieved 39\% node reduction and 49\% edge reduction
\end{itemize}

\textbf{Research Insights:}
\begin{itemize}
  \item Entry points are critical starting locations for CFG traversal, ensuring we analyze reachable code rather than data sections
  \item The normalize parameter in CFGFast is essential for creating clean, non-overlapping basic blocks suitable for GNN processing
  \item Including jumpkind in edge attributes enables richer semantic representation of control flow, potentially improving similarity detection
  \item Minimal test binaries significantly reduce CFG complexity, making validation faster and clearer
  \item angr-utils visualization provides intuitive understanding of CFG structure with color-coded edge types
\end{itemize}

\textbf{Next Steps:}
\begin{itemize}
  \item Full dataset processing: Re-run batch preprocessing on Dataset-1 to regenerate all JSONs with new jumpkind edge attribute
  \item Dataset loader update: Modify dataset/code\_dataset.py to parse jumpkind string and include as edge attributes in PyG data objects
  \item Consider edge type encoding: Evaluate whether one-hot encoding or simple integer enum is sufficient for GAT/GCN models
\end{itemize}



\subsection{Development Session: 2025-12-17}

\textbf{Summary:} Implementation and code improvements

\textbf{Duration:} 0h 5m

\textbf{Project Status - Completed Phases:}
\begin{itemize}
  \item Phase 1: Setup (T001-T009)
  \item Phase 2: Foundational - Test Binary Validation (T010-T014)
  \item Phase 3: User Story 1 - Preprocessing (T015-T026)
  \item Phase 4: User Story 2 - Dataset Implementation (T027-T036)
  \item Phase 5: User Story 3 - GNN Encoder (T037-T045)
  \item Phase 6: User Story 4 - Custom Attention (T046-T058)
  \item Phase 7: User Story 5 - BERT Integration (T059-T070)
  \item Phase 8: User Story 6 - Demonstration Notebook (T071-T073)
  \item Phase 9: User Story 7 - Training Infrastructure (T074-T089)
  \item Phase 10: User Story 8 - Vectorization & Inference (T090-T101)
\end{itemize}

\textbf{Implementation Work:}
\begin{itemize}
  \item What: A wrapper class for the pre-trained CLAP-ASM tokenizer from Hugging Face.
  \item Why: To provide a robust, assembly-aware BPE tokenizer that handles out-of-vocabulary terms better than a simple whitespace splitter.
  \item Uses `transformers.PreTrainedTokenizerFast`.
  \item Loads vocabulary from `preprocessing/clap\\_asm\\_tokenizer`.
  \item Joins instruction lists into a single string for tokenization (standard NLP approach).
  \item Returns `token\\_ids` and `attention\\_mask` compatible with our pipeline.
  \item Contribution to System: Improves the quality of input features for the BERT component of the BCSD model.
  \item Created `preprocessing/clap\\_asm\\_tokenizer/` to store artifacts.
\end{itemize}

\textbf{Architecture and Design Decisions:}
This section documents the architectural choices made during implementation,
including the rationale behind key decisions and their impact on the overall system.
\begin{itemize}
  \item Context: Direct usage of `AutoTokenizer.from\\_pretrained()` creates dependencies on external services and introduces interface mismatches with our existing pipeline. Options Considered: Decision: **Option 2 (Adapter Pattern)** Rationale:
\end{itemize}

\textbf{Decision Rationale and Justification:}
The following rationale guided implementation decisions:
\begin{itemize}
  \item **Domain Specificity**: Vocabulary includes `mov`, `rax`, `0x` etc. as whole tokens.
  \item **Robustness**: BPE handles arbitrary literals/labels via subword decomposition.
  \item **Integration**: Standard HF interface makes it easy to swap in.
  \item Downloaded vocab locally to avoid runtime dependency on HF Hub.
  \item Wrapped in `ClapASMTokenizer` to match our project's interface.
  \item **Offline Support**: Enables training/inference on isolated research clusters.
  \item **Security**: Avoids `trust\\_remote\\_code=True` by using the standard, safe `PreTrainedTokenizerFast` implementation with local data.
  \item Created `preprocessing/clap\\_asm\\_tokenizer/` to store artifacts.
  \item Implemented `ClapASMTokenizer.tokenize()` to handle list-to-string joining, effectively decoupling the preprocessing logic from the tokenization specifics.
  \item **Impact on system**:
\end{itemize}

\textbf{Research Insights:}
\begin{itemize}
  \item ClapASMTokenizer Performance**: Achieved >99.7% vocabulary coverage (only 0.27% UNK rate) on test binary `test_gnn_gcc_O0` without any fine-tuning.
\end{itemize}
