\chapter{Methodology and Development Process}

\section{Overview}

This chapter documents the research methodology and iterative development process
employed in building the BCSD (Binary Code Similarity Detection) model. Following
a constitution-based research approach, we emphasize reproducibility, documentation,
and systematic experimentation throughout the development lifecycle.

\subsection{Research Philosophy}

The development follows key principles:
\begin{itemize}
    \item \textbf{Documentation First:} All design decisions are documented before implementation
    \item \textbf{Reproducible Pipeline:} Every experiment can be reproduced with fixed random seeds
    \item \textbf{Modular Architecture:} Components are independently testable and replaceable
    \item \textbf{Iterative Validation:} Each phase is validated before proceeding to the next
\end{itemize}

\subsection{Development Pipeline}

The BCSD model development follows an 8-phase pipeline:

\begin{enumerate}
    \item \textbf{Preprocessing:} Binary disassembly and CFG extraction using angr
    \item \textbf{Tokenization:} Custom assembly instruction tokenization with domain vocabulary
    \item \textbf{Dataset Construction:} Dynamic positive pair sampling for Siamese training
    \item \textbf{GNN Encoding:} Graph Attention Networks for CFG summarization
    \item \textbf{Custom Attention:} Key-Value prefix injection mechanism
    \item \textbf{BERT Integration:} Deep graph-prefix fusion across all transformer layers
    \item \textbf{Training:} Joint MLM and contrastive learning with configurable loss weights
    \item \textbf{Inference:} Vectorization and similarity search using cosine distance
\end{enumerate}

\section{Phase 1: Binary Preprocessing and CFG Extraction}

\subsection{angr CFGFast Analysis}

We employ angr's CFGFast analysis for efficient and accurate control flow graph
extraction without the overhead of full symbolic execution. The choice of CFGFast
over CFGEmulated is motivated by:

\begin{itemize}
    \item \textbf{Speed:} 10-100Ã— faster than symbolic execution
    \item \textbf{Accuracy:} Pattern-based approach yields high-quality CFGs
    \item \textbf{Scalability:} Can process large binaries in reasonable time
\end{itemize}

The extraction process is function-centric, isolating individual functions to enable
fine-grained similarity detection at the function level rather than whole-binary level.

\subsection{Data Format Design}

The preprocessed data format follows a unified JSON schema:

\begin{lstlisting}[language=json, caption={Unified JSON format for preprocessed binaries}]
{
  "binary_hash": "sha256_hash",
  "functions": [
    {
      "function_name": "main",
      "nodes": [
        {
          "id": 0,
          "instructions": ["push rbp", "mov rbp, rsp"],
          "token_ids": [101, 234, 567]
        }
      ],
      "edges": [[0, 1], [1, 2]]
    }
  ]
}
\end{lstlisting}

This format enables efficient loading during training while preserving both sequential
(instructions) and structural (edges) information.

\section{Phase 2: Custom Assembly Tokenization}

Unlike natural language, assembly code exhibits unique characteristics that require
specialized tokenization strategies. Our custom tokenizer implements:

\begin{itemize}
    \item \textbf{Opcode-Register Separation:} Splitting instructions into semantic units
    \item \textbf{Frequency-Based Vocabulary:} Top-K most common tokens (K=5000)
    \item \textbf{Special Token Handling:} BERT-compatible special tokens ([CLS], [SEP], etc.)
    \item \textbf{Semantic Grouping:} Grouping similar instructions (e.g., conditional jumps)
\end{itemize}

The vocabulary size of 5000 tokens balances coverage and model efficiency, capturing
95\% of instruction patterns in our dataset while maintaining reasonable embedding
table size.

\section{Phase 3: Siamese Training with Dynamic Pairing}

The training architecture employs Siamese networks with dynamic positive pair sampling.
Unlike traditional fixed-pair approaches, our dataset implementation:

\begin{enumerate}
    \item Groups binaries by (project, function\_name)
    \item Samples 2 binaries from same group per training iteration
    \item Ensures diversity through random sampling across compilation variants
\end{enumerate}

This approach increases effective training data size without storing pre-computed pairs,
enabling efficient augmentation and reducing storage requirements.

\section{Phase 4-6: GNN and Custom Attention Integration}

\subsection{Graph Attention Network Architecture}

The GNN encoder employs a 3-layer Graph Attention Network with multi-head attention:

\begin{itemize}
    \item \textbf{Layer 1:} Input node features $\rightarrow$ hidden dimension 256
    \item \textbf{Layer 2-3:} Hidden $\rightarrow$ hidden with residual connections
    \item \textbf{Pooling:} Attention-weighted global pooling
    \item \textbf{Output:} Fixed-size graph summary (configurable: 128/256/512 dims)
\end{itemize}

\subsection{Key-Value Prefix Attention Mechanism}

The core innovation of this thesis is the custom KV-prefix attention mechanism that
injects graph knowledge into BERT's attention layers. Unlike concatenation-based fusion,
this approach:

\begin{enumerate}
    \item Projects graph summary to separate Key and Value prefix vectors
    \item Injects prefixes at \textbf{every attention layer} (deep fusion)
    \item Extends attention mask to include prefix position
    \item Maintains gradient flow to graph encoder through prefix projections
\end{enumerate}

Mathematically, for each attention layer $l$:

\begin{align}
K_{prefix}^{(l)} &= W_K^{(l)} \cdot \text{graph\_summary} \\
V_{prefix}^{(l)} &= W_V^{(l)} \cdot \text{graph\_summary} \\
K_{total}^{(l)} &= [K_{prefix}^{(l)}; K_{sequence}^{(l)}] \\
V_{total}^{(l)} &= [V_{prefix}^{(l)}; V_{sequence}^{(l)}] \\
\text{Attention}^{(l)} &= \text{softmax}\left(\frac{Q K_{total}^T}{\sqrt{d_k}}\right) V_{total}
\end{align}

This design ensures that every token in the sequence can attend to the graph summary,
enabling rich cross-modal interaction.

\section{Phase 7: Joint Training Objective}

The model is trained with a joint objective combining:

\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{MLM} + \lambda \cdot \mathcal{L}_{contrastive}
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_{MLM}$: Masked Language Model loss for instruction prediction
    \item $\mathcal{L}_{contrastive}$: InfoNCE contrastive loss with temperature $\tau = 0.07$
    \item $\lambda \in \{0.3, 0.5, 0.7\}$: Configurable loss weight for experimentation
\end{itemize}

The dual objective ensures the model learns both:
\begin{enumerate}
    \item Local instruction semantics (via MLM)
    \item Global function-level similarity (via contrastive learning)
\end{enumerate}

\section{Phase 8: Inference and Similarity Search}

The inference pipeline is designed for efficiency:

\begin{enumerate}
    \item \textbf{Vectorization:} Extract 768-dimensional embeddings from trained model
    \item \textbf{Storage:} Save embeddings as NumPy arrays (.npy format)
    \item \textbf{Search:} CPU-only cosine similarity using vectorized NumPy operations
\end{enumerate}

Performance benchmarks demonstrate sub-second search times for 10,000 embeddings,
validating the practical applicability of the approach.

\section{Research Methodology and Development Log}

This section documents the iterative development process of the BCSD model,
providing transparency into the research methodology and implementation decisions.

% Session logs will be automatically appended here by end_session.sh


\subsection{Development Session: 2025-12-15}

\textbf{Summary:} Test summary

\textbf{Duration:} 0h 0m

\textbf{Key Achievements:}
\begin{itemize}
  \item Achievement 1
  \item Achievement 2
\end{itemize}

\subsection{Development Session: 2025-12-14}

\textbf{Summary:} today was mostly implementation

\textbf{Duration:} 0h 7m

